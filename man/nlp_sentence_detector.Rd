% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sentence-detector.R
\name{nlp_sentence_detector}
\alias{nlp_sentence_detector}
\title{Spark NLP SentenceDetector - sentence boundary detector}
\usage{
nlp_sentence_detector(x, input_cols, output_col, custom_bounds = NULL,
  use_custom_only = NULL, use_abbreviations = NULL,
  explode_sentences = NULL, uid = random_string("sentence_detector_"))
}
\arguments{
\item{x}{A Spark connection, pipeline object, or a Spark data frame.}

\item{input_cols}{Input columns. Required.}

\item{output_col}{Output column. Required.}

\item{custom_bounds}{Custom sentence separator text. Optional.}

\item{use_custom_only}{Use only custom bounds without considering those of Pragmatic Segmenter. Defaults to false. Needs customBounds.}

\item{use_abbreviations}{Whether to consider abbreviation strategies for better accuracy but slower performance. Defaults to true.}

\item{explode_sentences}{Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false.}

\item{uid}{UID}
}
\value{
When \code{x} is a \code{spark_connection} the function returns a SentenceDetector Transformer. When
\code{x} is a \code{ml_pipeline} the pipeline with the SentenceDetector added. When \code{x} is a 
\code{tbl_spark} a transformed \code{tbl_spark} (note that the Dataframe passed in must contain the input_cols specified).
}
\description{
Spark ML Transformer that finds sentence bounds in raw text. Applies rule from Pragmatic Segmenter
See \url{https://nlp.johnsnowlabs.com/docs/en/annotators#sentencedetector}
}
