% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizer.R
\name{nlp_tokenizer}
\alias{nlp_tokenizer}
\title{Spark NLP Tokenizer approach}
\usage{
nlp_tokenizer(x, input_cols, output_col, exceptions = NULL,
  exceptions_path = NULL, case_sensitive_exceptions = NULL,
  context_chars = NULL, split_chars = NULL, target_pattern = NULL,
  suffix_pattern = NULL, prefix_pattern = NULL,
  infix_patterns = NULL, uid = random_string("tokenizer_"))
}
\arguments{
\item{x}{A Spark connection, pipeline object, or a Spark data frame.}

\item{input_cols}{Input columns. String array Required.}

\item{output_col}{Output column. String. Required.}

\item{exceptions}{String array. List of tokens to not alter at all. Allows composite tokens like two worded tokens that the user may not want to split.}

\item{exceptions_path}{NOTE: NOT IMPLEMENTED. String. Path to txt file with list of token exceptions}

\item{case_sensitive_exceptions}{Boolean. Whether to follow case sensitiveness for matching exceptions in text}

\item{context_chars}{String array. Whether to follow case sensitiveness for matching exceptions in text}

\item{split_chars}{String array.  List of 1 character string to rip off from tokens, such as parenthesis or question marks. Ignored if using prefix, infix or suffix patterns.}

\item{target_pattern}{String. Basic regex rule to identify a candidate for tokenization. Defaults to `\\S+` which means anything not a space}

\item{suffix_pattern}{String. Regex to identify subtokens that are in the end of the token. Regex has to end with `\\z` and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis}

\item{prefix_pattern}{String. Regex to identify subtokens that come in the beginning of the token. Regex has to start with `\\A` and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis}

\item{infix_patterns}{String array. extension pattern regex with groups to the top of the rules (will target first, from more specific to the more general).}

\item{uid}{UID}
}
\value{
When \code{x} is a \code{spark_connection} the function returns a ?????? Transformer. When
\code{x} is a \code{ml_pipeline} a Pipeline with the ?????? as the only stage. When \code{x} is a 
\code{tbl_spark} a transformed \code{tbl_spark}.
}
\description{
Spark ML estimator that identifies tokens with tokenization open standards. A few rules will help customizing
it if defaults do not fit user needs. See \url{https://nlp.johnsnowlabs.com/docs/en/annotators#tokenizer}
}
